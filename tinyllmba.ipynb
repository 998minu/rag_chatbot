{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.1.13\n",
        "!pip install langchain-openai==0.1.1"
      ],
      "metadata": {
        "id": "I3_Nwtwg5_g7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade unstructured"
      ],
      "metadata": {
        "id": "rD-0gH156vM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZZdMSs8weDN"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import DirectoryLoader\n",
        "from unstructured import partition\n",
        "directory = '/content/sample_data/content'\n",
        "\n",
        "def load_docs(directory):\n",
        "  loader = DirectoryLoader(directory)\n",
        "  documents = loader.load()\n",
        "  return documents\n",
        "\n",
        "documents = load_docs(directory)\n",
        "len(documents)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def split_docs(documents,chunk_size=300,chunk_overlap=10):\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "  docs = text_splitter.split_documents(documents)\n",
        "  return docs\n",
        "\n",
        "docs = split_docs(documents)\n",
        "print(len(docs))"
      ],
      "metadata": {
        "id": "OjZc0AQL7Hoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "24bcb8iw7WAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from chromadb.utils import embedding_functions\n",
        "default_ef = embedding_functions.DefaultEmbeddingFunction()"
      ],
      "metadata": {
        "id": "r5xnLk_Js3WU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "OQDsKX-X7Hlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "id": "rEgFz4mw8BAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "db = Chroma.from_documents(docs, embeddings)"
      ],
      "metadata": {
        "id": "Iv89lQ1o7Hiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the conculsion of story in 30 words?\"\n",
        "matching_docs = db.similarity_search(query)\n",
        "\n",
        "matching_docs[1]\n"
      ],
      "metadata": {
        "id": "K2RzduAH7Hf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "persist_directory = \"chroma_db\"\n",
        "\n",
        "vectordb = Chroma.from_documents(\n",
        "    documents=docs, embedding=embeddings, persist_directory=persist_directory\n",
        ")\n",
        "\n",
        "vectordb.persist()"
      ],
      "metadata": {
        "id": "e7uLuwVB7Hc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pypdf\n",
        "# !pip install -q llama-index\n",
        "!pip install -q gradio\n",
        "!pip install einops\n",
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "j9OUyjLxoK3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama_index==0.9.24"
      ],
      "metadata": {
        "id": "V5dfyupI7HaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index\n",
        "from llama_index.core import BaseQueryEngine\n",
        "from llama_index.llms import HuggingFaceLLM\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "zKhRJugF7HUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.prompts.prompts import SimpleInputPrompt\n",
        "\n",
        "system_prompt = \"You are a Q&A assistant. Your goal is to answer questions as accurately as possible based on the instructions and context provided.\"\n",
        "\n",
        "# This will wrap the default prompts that are internal to llama-index\n",
        "query_wrapper_prompt = SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")\n",
        "\n",
        "\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    context_window=2048,\n",
        "    max_new_tokens=256,\n",
        "    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
        "    system_prompt=system_prompt,\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    tokenizer_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    # uncomment this if using CUDA to reduce memory usage\n",
        "    model_kwargs={\"torch_dtype\": torch.bfloat16}\n",
        ")"
      ],
      "metadata": {
        "id": "7Rfat1Ib-Syp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r_fx2ch8-Sv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cgzjrw2K-Stf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2FFj8cBE-Sq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lC-11cNf-SoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "37A6YDks-SmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i41a3tFz-Siu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}