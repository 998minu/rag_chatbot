{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7x5itixs5Wb3"
      },
      "outputs": [],
      "source": [
        "!pip install llama_index==0.9.24"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pypdf\n",
        "!pip install -q python-dotenv\n",
        "# !pip install -q llama-index\n",
        "!pip install -q gradio\n",
        "!pip install einops\n",
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "ZB1ftO_a6BSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from llama_index.core import BaseQueryEngine\n",
        "# from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "# from llama_index.llms import HuggingFaceLLM\n",
        "# import torch"
      ],
      "metadata": {
        "id": "9FWZa5Td6M2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
        "from llama_index.llms import HuggingFaceLLM\n",
        "import torch\n",
        "\n",
        "documents = SimpleDirectoryReader(\"/content/Data\").load_data()"
      ],
      "metadata": {
        "id": "3ApgKi956BPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.prompts.prompts import SimpleInputPrompt\n",
        "\n",
        "system_prompt = \"You are a Q&A assistant. Your goal is to answer questions as accurately as possible based on the instructions and context provided.\"\n",
        "\n",
        "# This will wrap the default prompts that are internal to llama-index\n",
        "query_wrapper_prompt = SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")\n",
        "\n",
        "\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    context_window=2048,\n",
        "    max_new_tokens=256,\n",
        "    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
        "    system_prompt=system_prompt,\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    tokenizer_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    device_map=\"cuda\",\n",
        "    # uncomment this if using CUDA to reduce memory usage\n",
        "    model_kwargs={\"torch_dtype\": torch.bfloat16}\n",
        ")"
      ],
      "metadata": {
        "id": "9spfobSd6BMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings import HuggingFaceEmbedding\n",
        "\n",
        "# loads BAAI/bge-small-en\n",
        "# embed_model = HuggingFaceEmbedding()\n",
        "\n",
        "# loads BAAI/bge-small-en-v1.5\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    chunk_size=1024,\n",
        "    llm=llm,\n",
        "    embed_model=embed_model\n",
        ")\n"
      ],
      "metadata": {
        "id": "jzLk3QOT6BJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "\n",
        "query_engine = index.as_query_engine()\n",
        "\n",
        "def predict(input, history):\n",
        "  response = query_engine.query(input)\n",
        "  return str(response)"
      ],
      "metadata": {
        "id": "hI6tFlq46BGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "gr.ChatInterface(predict).launch(share=True)"
      ],
      "metadata": {
        "id": "40mfVEED6BEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dRdGU7uk6BBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "aKzoUNB46A-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!dir"
      ],
      "metadata": {
        "id": "fsuXENsj6A8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls sample_data/\n"
      ],
      "metadata": {
        "id": "Vvv2CPtO6A5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iJiegpN86A29"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}